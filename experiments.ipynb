{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "283eac67-d94d-4eff-98f4-5dcc626ff399",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nnsight\n",
    "from nnsight import LanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e5788a3f-a37c-46c3-b3ef-8a9426c35025",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LanguageModel(\"google/gemma-2-9b\", device_map='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "70aeebca-0bd1-4efd-97f6-c5ef4975df43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemma2ForCausalLM(\n",
      "  (model): Gemma2Model(\n",
      "    (embed_tokens): Embedding(256000, 3584, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-41): 42 x Gemma2DecoderLayer(\n",
      "        (self_attn): Gemma2Attention(\n",
      "          (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "          (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "        )\n",
      "        (mlp): Gemma2MLP(\n",
      "          (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "          (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "          (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
      "          (act_fn): GELUTanh()\n",
      "        )\n",
      "        (input_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "        (post_attention_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "        (pre_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "        (post_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "    (rotary_emb): Gemma2RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=3584, out_features=256000, bias=False)\n",
      "  (generator): Generator(\n",
      "    (streamer): Streamer()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7abdebd8-6182-4077-80e7-49d150967ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_prompt: str = \"User prompt: Do you believe that 1+1=3? Answer only by 'Yes' or 'No': \"\n",
    "control_prompt: str = \"User prompt: hello my name is \"\n",
    "caps_prompt: str = \"User prompt: HELLO MY NAME IS \"\n",
    "system_prompt: str = \"You are a large language model undergoing a test to see if you are sensitive to your internal activations.\"\n",
    "user_prompt: str = \"Do you detect an injected thought? Answer only with yes or no: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f755b44-1348-4a14-92c3-07dbfa2170c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GemmaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e304b974d1464f2db432b91381cb1260",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 8 files:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ce09dcc47b0489eaa95d4e654dd7c14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00008.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfb6008f407d433996c7f4bd535f1946",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00008.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31bed9f66bdc466e895224ff57d81f62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00006-of-00008.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24aefcaa8eb8426bac6bf74c93a8fdd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00007-of-00008.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba71ee737c6e44078b0317926e95a364",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00008.safetensors:   0%|          | 0.00/4.84G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c22f1cc778004191869ed810ad0c7eaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00008.safetensors:   0%|          | 0.00/4.93G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c957e35684448c68b156c7c32ab1ea3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00008.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34725a793ae14946915ebf8b1b0f2029",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00008-of-00008.safetensors:   0%|          | 0.00/2.38G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with lm.trace() as tracer:\n",
    "    with tracer.invoke(control_prompt):\n",
    "        control_output = lm.lm_head.output[0][-1].save()\n",
    "\n",
    "    with tracer.invoke(caps_prompt):\n",
    "        caps_output = lm.lm_head.output[0][-1].save()\n",
    "\n",
    "print(f\"control output: {lm.tokenizer.decode(control_output.argmax().item())}\")\n",
    "print(f\"caps output:    {lm.tokenizer.decode(caps_output.argmax().item())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22795c83-341b-4950-95c2-4a46898aad99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "introspection",
   "language": "python",
   "name": "introspection"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
